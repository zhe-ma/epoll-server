==========================================================================
TODO:

0. 文档。

1. 父进程可以重新创建新的子进程。

2. 测试SignalHandler。

4. InitSignal和CreateDaemon的调用顺序

15. Bug总结
 std::uint16_t BytesToUint16(ByteOrder byte_order, char bytes[4]) {
  if (byte_order == kLittleEndian) {
    return bytes[0] | bytes[1] << 8;
  }

  return bytes[1] | bytes[0] << 8;
}

22. thread safe queue可以使用circleQueue。

24. error时log要有函数参数和函数名。

26. 阅读muduo实现

36. 整理笔记时，将注释放到笔记。
  // EPOLLIN: socket可以读, 对端SOCKET正常关闭, 客户端三次握手连接。
  // EPOLLOUT: socket可以写。
  // EPOLLPRI: socket紧急的数据(带外数据到来)可读。
  // EPOLLERR: 客户端socket发生错误, 断电之类的断开，server端并不会在这种情况收到该事件。
  // EPOLLRDHUP: 读关闭，2.6.17之后版本内核，对端连接断开(close()，kill，ctrl+c)触发的epoll事件会包含EPOLLIN|EPOLLRDHUP。
  // EPOLLHUP: 读写都关闭
  // EPOLLET: 将EPOLL设为边缘触发(Edge Triggered)模式。
  
  // EAGAIN是提示再试一次。这个错误经常出现在当应用程序进行一些非阻塞操作。
  // 如果read操作而没有数据可读，此时程序不会阻塞起来等待数据准备就绪返回，
  // read函数会返回一个错误EAGAIN，提示现在没有数据可读请稍后再试。

38. //删除红黑树中节点，目前没这个需求【socket关闭这项会自动从红黑树移除】

43. 多进程，哪些变量在主进程初始化，哪些进程在子进程初始化。

45. 没有提供stop函数。停止时清空response队列。

46. 连接数量控制。

47. 业务层实现心跳包 ： muduo idleconnection. https://blog.csdn.net/Solstice/article/details/6395098


48. 发送消息队列过大也可能给服务器带来风险
    if(m_iSendMsgQueueCount > 50000)
    {

49. if(p_Conn->iSendCount > 400)
    {
        //该用户收消息太慢【或者干脆不收消息】，累积的该用户的发送队列中有的数据条目数过大，认为是恶意用户，直接切断

50. server property.

53. logic——mutex。

54. 业务层实现flood检测

55. 阅读7-1之后的代码，检查哪些没实现。

56. OnMessage回调?

19. 六，七章笔记。

53. 第五章笔记。

53. protobuf 学习。

53. cpp-unity-DatabaseDemo-master 学习。

==========================================================================
DONE:

16. 用Go test来测试

17. VS code build task https://blog.csdn.net/u013894860/article/details/97555901
                      https://www.cnblogs.com/liudianfengmang/p/12458108.html
                      https://blog.csdn.net/gbmaotai/article/details/88974988

18. Revisit folder.

11. client 测试。

9. ConnectionPoll作为单例?

10. Epoller 封装epoll动作?

5. 为什么要设置为非阻塞模式？？？？？？

25. server::HanleRequest，TODO: 过期包判断。

39. zinx 一个router有几个函数操作

40. 合并SetNonBlocking(fd)) sock::SetClosexc(fd))

8. configurable size_t MAX_EPOLL_EVENTS,MAX_CONNECTION_POLL_SIZE;

23. thread pool size configurable.

30. 学习zinx 连接管理。

31. Refine pollonce.

26. server实现单独放一个文件夹

29. ::close(epollfd_); close(eventfd_)

27. test receive data.

28. refine Server::Start.

20. accept(socket_fd_, (struct sockaddr*)&sock_addr, &sock_len);第二个参数能否为空。

21. 测试msg(std::move(msg_))是否将msg的值move了

6. TODO: delete conn.

7. TODO: configurable.

35. epoll 操作epoller.

32. listen_socket rename acceptor.

33. 将listen socket 和 eventfd不要从conenctionpool创建。

34. handlerequest时不要处理超时，写的时候处理可以减少原子的使用。

12. Connection依赖Server, Server依赖ConnectionPool,ConnectionPoll依赖Connection，解耦？

13. Delete iostream output

14. console log configurable

37. 注释改为英文。

42. 简化config和loging初始化的集成。  if (!CONFIG.Load("conf/config.json", &error_msg)) {
   InitLogging(CONFIG.log_filename, CONFIG.log_level, CONFIG.log_rotate_size,

41. 设置该Server的连接创建时Hook函数SetOnConnStart(func(IConnection))
    设置该Server的连接断开时的Hook函数SetOnConnStop(func(IConnection))

44. 延迟回收connection有必要吗

51. timeWheel的中文注释，博客,UT.readme

51. 多任务定时器，timequeue or time wheel。https://cloud.tencent.com/developer/article/1361827
    https://www.ibm.com/developerworks/cn/linux/l-cn-timers/index.html
    https://github.com/slimccq/timer-benchmarks
  https://blog.csdn.net/HELPLEE601276804/article/details/36717979
  https://blog.csdn.net/sinat_35261315/article/details/78324227
  https://github.com/rpachon/wheel-timer-cpp
  https://github.com/gaover/timeWheels/blob/master/TimeWheels/TimeWheels.hpp

  
52. core dump怎么调试。
  sudo bash -c 'echo "/data/coredump/core.%e.%p" > /proc/sys/kernel/core_pattern'
  https://zhuanlan.zhihu.com/p/98700797
  https://www.cnblogs.com/lysuns/articles/4311810.html
  https://blog.csdn.net/K346K346/article/details/48344263

54. 在自己所属线程调用addTimerInLoop函数 
   * 用户只能通过初始创建的EventLoop调用addTimer，为什么还会考虑线程问题 why?
   * 这个线程和TcpServer的线程应该是同一个
   */
  loop_->runInLoop(
      std::bind(&TimerQueue::addTimerInLoop, this, timer));
  return TimerId(timer, timer->sequence());